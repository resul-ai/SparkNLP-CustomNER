{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russell-ai/SparkNLP-CustomNER/blob/main/1_Pipeline_Implementation_with_Chunk_Merger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I08sFJYCxR0Z"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWu_IMfY4L1N"
      },
      "source": [
        "# **Interview Task**\n",
        "[Running a Spark NLP Healthcare Pipeline and Training a Custom NER Model](https://docs.google.com/document/d/1l_SpYGAlVGAEe9x-b8avgvKipCXetdap2ttc4UKreO4/edit?tab=t.0)  \n",
        "## **PART-I Pipeline Implementation:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okhT7AcXxben"
      },
      "source": [
        "## 1. Set Up Spark NLP for Healthcare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REmprZQ8Mbrr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "if 'spark_jsl.json' not in os.listdir():\n",
        "  license_keys = files.upload()\n",
        "  os.rename(list(license_keys.keys())[0], 'spark_jsl.json')\n",
        "\n",
        "with open('spark_jsl.json') as f:\n",
        "    license_keys = json.load(f)\n",
        "\n",
        "# Defining license key-value pairs as local variables\n",
        "locals().update(license_keys)\n",
        "os.environ.update(license_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiigSdavGaeg"
      },
      "outputs": [],
      "source": [
        "license_keys.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIjl47npIyCK"
      },
      "outputs": [],
      "source": [
        "license_keys['JSL_VERSION']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL465qu3GKEG"
      },
      "outputs": [],
      "source": [
        "license_keys['PUBLIC_VERSION']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wRTzAtKjXpo"
      },
      "outputs": [],
      "source": [
        "# Installing pyspark and spark-nlp\n",
        "! pip install --upgrade -q pyspark==3.4.1 spark-nlp==$PUBLIC_VERSION\n",
        "\n",
        "\n",
        "# Installing Spark NLP Healthcare\n",
        "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
        "\n",
        "# Installing Spark NLP Display Library for visualization\n",
        "! pip install -q spark-nlp-display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziwjuhAV7DYH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import sparknlp\n",
        "import sparknlp_jsl\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp_jsl.pipeline_tracer import PipelineTracer\n",
        "from sparknlp_jsl.pipeline_output_parser import PipelineOutputParser\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "params = {\"spark.driver.memory\":\"16G\",\n",
        "          \"spark.kryoserializer.buffer.max\":\"2000M\",\n",
        "          \"spark.driver.maxResultSize\":\"2000M\"}\n",
        "\n",
        "print(\"Spark NLP Version :\", sparknlp.version())\n",
        "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
        "\n",
        "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l4VvgI1dRb6"
      },
      "outputs": [],
      "source": [
        "from sparknlp_jsl.pretrained import InternalResourceDownloader\n",
        "\n",
        "ner_models = InternalResourceDownloader.returnPrivateModels(\"MedicalNerModel\")\n",
        "for model, lang, version in ner_models:\n",
        "  if lang == \"en\" and (model.startswith(\"ner_clinical\") or model.startswith(\"ner_posology\")):\n",
        "    print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Selection"
      ],
      "metadata": {
        "id": "ndUm4nh6cCrr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1lmxbPt1dZW"
      },
      "outputs": [],
      "source": [
        "# mt_samples dataset from John Snow Labs\n",
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/mt_samples_10.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1S9J6CASrSE"
      },
      "outputs": [],
      "source": [
        "mt_samples_df = spark.read.csv(\"mt_samples_10.csv\", header=True, multiLine=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt_9Qfmw30rA"
      },
      "outputs": [],
      "source": [
        "mt_samples_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te55tz4dwAB5"
      },
      "outputs": [],
      "source": [
        "mt_samples_df.show(truncate=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5soib4Tw4PK"
      },
      "outputs": [],
      "source": [
        "print(mt_samples_df.limit(1).collect()[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vWIDpk50QfG"
      },
      "source": [
        "## 3. NER Pipeline Execution:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline"
      ],
      "metadata": {
        "id": "cqd5lRNoe2bR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd3wwfajfyDb"
      },
      "outputs": [],
      "source": [
        "# Document assembler\n",
        "document_assembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Sentence detector\n",
        "sentence_detector = SentenceDetector()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"sentence\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Word Embeddings\n",
        "embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"sentence\", \"token\"])\\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "# NER models\n",
        "ner_clinical = MedicalNerModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
        "    .setOutputCol(\"ner_clinical\")\n",
        "\n",
        "ner_posology_greedy = MedicalNerModel.pretrained(\"ner_posology_greedy\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
        "    .setOutputCol(\"ner_posology\")\n",
        "\n",
        "ner_deid_generic = MedicalNerModel.pretrained(\"ner_deid_generic_augmented\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
        "    .setOutputCol(\"ner_deid\")\n",
        "\n",
        "# NER Converters\n",
        "ner_conv_clinical = NerConverterInternal()\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"ner_clinical\"])\\\n",
        "    .setOutputCol(\"ner_chunk_clinical\")\n",
        "\n",
        "ner_conv_posology = NerConverterInternal()\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"ner_posology\"])\\\n",
        "    .setOutputCol(\"ner_chunk_posology\")\\\n",
        "    .setWhiteList([\"DRUG\"])\n",
        "\n",
        "ner_conv_deid = NerConverterInternal()\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"ner_deid\"])\\\n",
        "    .setOutputCol(\"ner_chunk_deid\")\\\n",
        "    .setWhiteList([\"NAME\", \"DATE\"])\n",
        "\n",
        "# Chunk Merger\n",
        "chunk_merger = ChunkMergeApproach()\\\n",
        "    .setInputCols([\"ner_chunk_clinical\", \"ner_chunk_posology\", \"ner_chunk_deid\"])\\\n",
        "    .setOutputCol(\"merged_chunks\")\\\n",
        "    .setMergeOverlapping(True)\n",
        "\n",
        "# Pipeline Creation\n",
        "pipeline = Pipeline(\n",
        "    stages=[\n",
        "        document_assembler,\n",
        "        sentence_detector,\n",
        "        tokenizer,\n",
        "        embeddings,\n",
        "        ner_clinical,\n",
        "        ner_posology_greedy,\n",
        "        ner_deid_generic,\n",
        "        ner_conv_clinical,\n",
        "        ner_conv_posology,\n",
        "        ner_conv_deid,\n",
        "        chunk_merger\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test pipeline"
      ],
      "metadata": {
        "id": "HNiCVooVc_P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the pipeline to an empty dataframe\n",
        "empty_df = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "model = pipeline.fit(empty_df)"
      ],
      "metadata": {
        "id": "_6U3FwscqDoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V81zaAe13qLU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Run the pipeline\n",
        "text = \"Patient John Doe was prescribed 500mg of Aspirin on 2023-05-15 for his chronic pain.\"\n",
        "test_df = spark.createDataFrame([(text,)]).toDF(\"text\")\n",
        "results = model.transform(test_df)\n",
        "\n",
        "# Display results\n",
        "results.select(\"merged_chunks.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract **ner_clinical** predictions from the mt_samples via the pipeline"
      ],
      "metadata": {
        "id": "9Fua5HaEddmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare input texts"
      ],
      "metadata": {
        "id": "_BCuobe1f8yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# total_rows = mt_samples_df.count()\n",
        "# texts = [mt_samples_df.select(\"text\").collect()[i]['text'] for i in range(total_rows)]\n",
        "texts = mt_samples_df.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
        "print(len(texts))\n",
        "print(type(texts), type(texts[0]))"
      ],
      "metadata": {
        "id": "t3nBISDV0zf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, text in enumerate(texts):\n",
        "  print(f\"Text {idx + 1}:\".center(100, '-'))\n",
        "  print(text)\n",
        "  if idx == 2:\n",
        "    break"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qKTHJn5Jwa81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **pipeline_tracer** to get structured output"
      ],
      "metadata": {
        "id": "UgjtI1-nelBd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzTqEZSUNfVi"
      },
      "outputs": [],
      "source": [
        "pipeline_tracer = PipelineTracer(light_pipeline_model)\n",
        "\n",
        "column_maps = pipeline_tracer.createParserDictionary()\n",
        "column_maps.update({\"document_identifier\": \"ner_pipeline\"})\n",
        "pipeline_parser = PipelineOutputParser(column_maps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting prediction"
      ],
      "metadata": {
        "id": "oCfC6BnDfDqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "import pandas as pd\n",
        "\n",
        "# This Empty list stores the results\n",
        "all_results = []\n",
        "all_texts = []\n",
        "\n",
        "# Apply the pipeline to each text\n",
        "for idx, text in enumerate(texts):\n",
        "    light_result = light_model.fullAnnotate([text])\n",
        "    result = pipeline_parser.run(light_result)\n",
        "\n",
        "    # add the results to the list\n",
        "    for entity in result['result'][0]['entities']:\n",
        "        all_results.append({\n",
        "            'text_id': idx,\n",
        "            'begin': entity['begin'],\n",
        "            'end': entity['end'],\n",
        "            'chunk': entity['chunk'],\n",
        "            'entity': entity['ner_label']\n",
        "        })\n",
        "\n",
        "    all_texts.append({'text_id': idx,'text': text})\n",
        "\n",
        "# convert the list to a pandas dataframe\n",
        "result_df = pd.DataFrame(all_results)\n",
        "text_df = pd.DataFrame(all_texts)"
      ],
      "metadata": {
        "id": "lJ1G97uoWhGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.head(10)"
      ],
      "metadata": {
        "id": "YkcBwydgYrZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_counts = result_df['entity'].value_counts()\n",
        "print(entity_counts)"
      ],
      "metadata": {
        "id": "lKK5rVs0hOnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save** the results of the NER and the corresponding texts as a csv file."
      ],
      "metadata": {
        "id": "pD4izrDGiEMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.to_csv(\"/content/ner_clinical_mtsamples_ner_results_for_conll.csv\", index=False)\n",
        "text_df.to_csv(\"/content/mtsamples_texts.csv\", index=False)"
      ],
      "metadata": {
        "id": "3nxpiahqWiFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test ner_posology pipeline"
      ],
      "metadata": {
        "id": "gWA0l4dZsZ_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text ='''The patient was prescribed 1 capsule of Parol with meals .\n",
        "He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day .\n",
        "It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months .'''"
      ],
      "metadata": {
        "id": "TEEyQVjhs8UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = 'embeddings_clinical'\n",
        "model_name = 'ner_posology'\n",
        "\n",
        "light_pipeline_model = get_pipeline_model(embeddings, model_name)\n",
        "light_model = LightPipeline(light_pipeline_model)\n",
        "light_result = light_model.fullAnnotate(text)"
      ],
      "metadata": {
        "id": "_nvYWXmMsZ_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(light_result)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9yANVvAc4KWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract **ner_posology** predictions from the mt_samples dataset via the pipeline"
      ],
      "metadata": {
        "id": "1AcTW0vCmlPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input texts for pipeline\n",
        "texts = mt_samples_df.select(\"text\").withColumn(\"text_id\", monotonically_increasing_id()).collect()"
      ],
      "metadata": {
        "id": "QAAWIjKkmrFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KMcquCDAmq0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store results\n",
        "all_results = []\n",
        "all_texts = []\n",
        "\n",
        "# Process each text through the pipeline\n",
        "for row in texts:\n",
        "  text_id = row['text_id']\n",
        "  text = row['text']\n",
        "\n",
        "  # Save the text and its id\n",
        "  all_texts.append({'text_id': int(text_id),'text': text})\n",
        "\n",
        "  # Run the posology NER pipeline\n",
        "  light_result = light_model.fullAnnotate([text])\n",
        "  result = pipeline_parser.run(light_result)\n",
        "\n",
        "  # Create a row for each entity\n",
        "  for entity in result['result'][0]['entities']:\n",
        "      all_results.append({\n",
        "          'text_id': int(text_id),\n",
        "          'begin': entity['begin'],\n",
        "          'end': entity['end'],\n",
        "          'chunk': entity['chunk'],\n",
        "          'entity': entity['ner_label']\n",
        "      })\n",
        "\n",
        "# Convert results to DataFrames\n",
        "result_df = pd.DataFrame(all_results)\n",
        "texts_df = pd.DataFrame(all_texts)\n",
        "\n",
        "# Save as CSV files\n",
        "result_df.to_csv(f\"ner_posology_mtsamples_ner_results.csv\", index=False)\n",
        "texts_df.to_csv(f\"mtsamples_texts.csv\", index=False)"
      ],
      "metadata": {
        "id": "q0bDpksz4wJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_df.iloc[0]['text']"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xsyn0DC2A-pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.head(10)"
      ],
      "metadata": {
        "id": "fDANn1GSmqwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df[\"entity\"].value_counts()"
      ],
      "metadata": {
        "id": "sngIJ8s19pTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---  \n",
        "*`R.Caliskan`*\n"
      ],
      "metadata": {
        "id": "GeFyGbHW_JWW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "okhT7AcXxben",
        "ndUm4nh6cCrr",
        "9Fua5HaEddmN",
        "_BCuobe1f8yJ",
        "UgjtI1-nelBd",
        "oCfC6BnDfDqw",
        "pD4izrDGiEMj",
        "gWA0l4dZsZ_n",
        "1AcTW0vCmlPd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "3f47d918ae832c68584484921185f5c85a1760864bf927a683dc6fb56366cc77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}